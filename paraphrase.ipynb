{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"paraphrase.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNEGiHPFGwgxPWK93YfY/cH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UPDZDRHobta7"},"source":["# Requirements"]},{"cell_type":"code","metadata":{"id":"k78mBboebdGw"},"source":["from pathlib import Path\n","kaggle = Path(\"/content/kaggle.json\")\n","\n","!sudo apt-get install -y git-lfs\n","!pip install wandb\n","!pip install kaggle\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install pytorch_lightning\n","!pip install madgrad\n","!git clone https://github.com/bloodwass/mixout\n","!git config --global user.email \"simonmeoni@aol.com\"\n","!git config --global user.name \"Simon Meoni\"\n","%run /content/mixout/mixout.py\n","%run /content/mixout/module.py\n","\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle competitions download -c commonlitreadabilityprize\n","!kaggle datasets download simonmeoni/litbank\n","!kaggle datasets download simonmeoni/paraphrase-clrp\n","!unzip train.csv.zip\n","!unzip paraphrase-clrp.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cwyfc1WAbwWQ"},"source":["import pandas as pd\n","import os\n","import torch\n","import gc\n","import pytorch_lightning as pl\n","import madgrad\n","import wandb\n","import math\n","import numpy as np\n","from torch import nn\n","import torch.nn.functional as F\n","import concurrent.futures\n","from transformers import (\n","    AutoTokenizer,\n","    AlbertModel,\n","    AutoModel,\n","    AdamW,\n","    AutoModelForMaskedLM,\n","    LineByLineTextDataset,\n","    DataCollatorForLanguageModeling,\n","    TrainingArguments,\n","    Trainer,\n","    AutoModelForSeq2SeqLM,\n",")\n","import transformers\n","from torchvision import transforms, utils\n","from pytorch_lightning.loggers import WandbLogger\n","from torch.utils.data import Dataset, DataLoader, Subset, random_split\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks.stochastic_weight_avg import StochasticWeightAveraging\n","from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n","from pytorch_lightning import Trainer\n","import glob\n","import random\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm.notebook import tqdm\n","from torch.optim.lr_scheduler import CosineAnnealingLR"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4h1Ua4Vb0lI"},"source":["# Paraphrase Dataset"]},{"cell_type":"code","metadata":{"id":"DwTSvrZ9b7Hw"},"source":["dataset = pd.read_csv('train.csv')\n","tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to('cuda')\n","for index, row in dataset.iterrows():\n","  sentence = row['excerpt']\n","  encoding = tokenizer.encode_plus(sentence, return_tensors=\"pt\")\n","  input_ids, attention_masks = encoding[\"input_ids\"].to('cuda'), encoding[\"attention_mask\"].to('cuda')\n","  outputs = model.generate(\n","      input_ids=input_ids, attention_mask=attention_masks,\n","      max_length=256,\n","      do_sample=True,\n","      top_k=120,\n","      top_p=0.95,\n","      early_stopping=True,\n","      num_return_sequences=5\n","  )\n","  dataset.at[index, 'excerpt'] = tokenizer.decode(random.choice(outputs), skip_special_tokens=True)\n","dataset.to_csv('paraphrase.csv')\n","!mkdir paraphrase-clrp \n","!mv paraphrase.csv /content/paraphrase-clrp/paraphrase.csv \n","!kaggle datasets metadata -p /content/paraphrase-clrp simonmeoni/paraphrase-clrp"],"execution_count":null,"outputs":[]}]}